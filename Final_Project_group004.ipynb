{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COGS118A/Group004-Sp22/blob/Dhruva's-branch-Testing/Final_Project_group004.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFejFA_PdYWG"
      },
      "source": [
        "# COGS 118A- Project Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-sgSEuxdYWP"
      },
      "source": [
        "# Names\n",
        "\n",
        "- Daniel Milton\n",
        "- Isabella Gonzalez\n",
        "- Dhruva Kolikineni\n",
        "- Harini Adivikolanu\n",
        "- Brandon Rocchio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollXQi7qdYWQ"
      },
      "source": [
        "# Abstract \n",
        "Classification involves predicting discrete class labels for unlabeled data. given information on the data. The data we are working with is information from individuals' shopping trips at Walmart. The data is broken down into 7 observations, one of which is the trip type which tells us what type of shopping trip this customer was on, visit number which organizes the data into individual shopping trips, weekday that the trip was done on, UPC number of the item purchased, department of purchase and the fineline number which is a number that Walmart made helping us specify the items purchased. Given certain data such as what weekday it is, what department it was in, and what the UPC was, our goal is to be able to predict what type of shopping trip someone was on based off of a couple pieces of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsSiF2NbdYWR"
      },
      "source": [
        "# Background\n",
        "\n",
        "Although supervised classification methods have been studied greatly throughout the years, research specific to this problem, on classifying a grocery trip based on the items bought remains low. There are multiple classification algorithms we can attempt in order to classify the 38 different types of shopping trips there are. K-nearest neighbors, decision trees, random forest classifiers, neural networks and logistic regression are commonly used in order to solve classification problems. Due to the low amount of research done in this specific problem area, this background section will be a small literature review of potential classification algorithms to use for our problem.\n",
        "\n",
        "Firstly, K Nearest Neighbors works based on the idea that for a target variable, the k number of patterns nearest to that target variable can provide useful information in order to properly classify the target variable. KNN assigns the target variable the classification of the majority of the nearest neighbors<a name=\"first\"></a>[<sup>[1]</sup>](#firstnote). The downfalls of KNN are that there is no right 'K' to choose and it can be computationally inefficient. Second, Decision Trees are another classification method we want to attempt, these are popular due to their good accuracy scores and their computational efficiency<a name=\"second\"></a>[<sup>[2]</sup>](#secondnote). The random forest classifier works by using multiple tree classifiers where each classifier is generated by using a random vector and each tree vottes for the most 'popular' class to classify an input vector<a name=\"third\"></a>[<sup>[3]</sup>](#thirdnote). This paper uses random forests to classify remote sensing which they concluded was just as accurate as using a support vector machine.\n",
        "\n",
        "Neural Networks and logistic regression are other potential algorithms we would like to try to classif our data with, there was no literature on any similar classification task to our project but a paper revealed that these two algorithms share common roots in statistical pattern recognition and that neural networks can be seen as a type of generalization from logistic regression<a name=\"fourth\"></a>[<sup>[4]</sup>](#fourthnote). Lastly, upon multiple attempts to find related work, Cui et al attempted this trip type classification using deep embedding logistic regression which incorporates logistic regression into a deep and narrow neural network<a name=\"fifth\"></a>[<sup>[5]</sup>](#fifthnote). We are hoping as we implement some of these algorithms to produce results and a discussion that can help future research for stores like Walmart to improve customers' shopping experiences or help understand/solve problems similar to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ1qnjctdYWS"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "Walmart currently employs a proprietary method to catogorize shopping trips into 38 distinct types. We have set out to create a clustering/catagorization model that, given a limited set of customer behavior features, predicts the shopping trip types.\n",
        "\n",
        "As an example for what these trip types may be: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj6srIgKvVLV"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-iNutKkdjao"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import *\n",
        "import io\n",
        "import tensorflow as tp\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KShpUbledYWT"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUEy9KryYal"
      },
      "source": [
        "## Data Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "zAbrEZjvyb5F",
        "outputId": "7aabc026-a751-439b-f533-553420a4f332"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-82056695a1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Load in train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Drop Upc column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#train.drop(columns=['Upc'], inplace = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ],
      "source": [
        "## Load in train dataset\n",
        "train = pd.read_csv((r'/content/train.csv'))\n",
        "\n",
        "## Drop Upc column\n",
        "#train.drop(columns=['Upc'], inplace = True)\n",
        "\n",
        "\n",
        "## Find all rows with null values\n",
        "null_vals = train[train.isna().any(axis=1)]\n",
        "\n",
        "## I found from the above code that there is 4129 rows\n",
        "## with null values. From this I found that Department Descriptions of\n",
        "## PHARMACY RX always has a null FinelineNumber\n",
        "\n",
        "\n",
        "## A check of all unique values in these columns to make sure if any\n",
        "## further cleaning of the strings is needed\n",
        "department_unique = train['DepartmentDescription'].unique()\n",
        "weekday_unique = train['Weekday'].unique()\n",
        "scancount_unique = train['ScanCount'].unique()\n",
        "\n",
        "## Function to change the week days to quantitative variables, which allows us\n",
        "## to do analysis on the Weekday column\n",
        "def weekday_int_converter(x):\n",
        "  if x=='Monday':\n",
        "    return 0\n",
        "  elif x=='Tuesday':\n",
        "    return 1\n",
        "  elif x=='Wednesday':\n",
        "    return 2\n",
        "  elif x=='Thursday':\n",
        "    return 3\n",
        "  elif x=='Friday':\n",
        "    return 4\n",
        "  elif x=='Saturday':\n",
        "    return 5\n",
        "  elif x=='Sunday':\n",
        "    return 6 \n",
        "\n",
        "\n",
        "train['Weekday'] = train['Weekday'].apply(weekday_int_converter)\n",
        "\n",
        "## Filtered the train csv of all null values in the Department \n",
        "## Description column\n",
        "train = train[train['DepartmentDescription'].notna()]\n",
        "\n",
        "\n",
        "## We found there to be 60367 duplicate rows.\n",
        "duplicates = train[train.duplicated()]\n",
        "\n",
        "## We decided not to drop the duplicates as they may provide valuable data.\n",
        "#train_duplicates_dropped = train.drop_duplicates()\n",
        "\n",
        "\n",
        "## Add a column indicated if the trip includes return\n",
        "train['Return'] = train['ScanCount']\n",
        "train\n",
        "def convert(x):\n",
        "    if x >= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "train['Return'] = train['ScanCount'].apply(convert)\n",
        "train\n",
        "## Test any variable to see results\n",
        "train.shape\n",
        "print(train.head())\n",
        "print(train[\"DepartmentDescription\"].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgTY9ILkIYIG"
      },
      "source": [
        "In order to clean our data, we imported the training data set, dropped the UPC column since we determined it was not a significant variable, changed weekdays into numerical categories, dropped all NA values as well as duplicate rows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMw1wAngC359"
      },
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVoQCdKQVI7t"
      },
      "outputs": [],
      "source": [
        "#temp = pd.get_dummies(train, columns=['DepartmentDescription'])\n",
        "#temp  = temp.dropna()\n",
        "#temp.isnull().values.any()\n",
        "#temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Create a new df to append a groupby series of the sum of items bought in each trip\n",
        "items_bought_each_trip = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "## Groupby function asssigned to new col\n",
        "items_bought_each_trip['item_sum'] = train.groupby(['VisitNumber'])['ScanCount'].sum()\n",
        "\n",
        "\n",
        "## This merge creates a df that makes only one row for each trip containing its sum of items\n",
        "merged = items_bought_each_trip.reset_index().merge(train[['TripType', 'VisitNumber', 'Weekday']].drop_duplicates(), on='VisitNumber', how = 'left')\n",
        "\n",
        "\n",
        "\n",
        "## Another dataframe to OHE departments\n",
        "new_df = train.groupby(['VisitNumber', 'DepartmentDescription'])['ScanCount'].sum().reset_index()\n",
        "\n",
        "## OHE\n",
        "temp = pd.get_dummies(cat, columns=['DepartmentDescription'])\n",
        "\n",
        "## THis bit of code I iterated throught the department desciption columns and made an\n",
        "## aggregate function to make only one row per trip with each deparment description it \n",
        "## had showing as one\n",
        "new_dict = {}\n",
        "for i in temp:\n",
        "    new_dict[i] = 'sum'\n",
        "final = temp.groupby(temp['VisitNumber']).aggregate(new_dict)\n",
        "final = final.iloc[: , 1:].reset_index()\n",
        "\n",
        "\n",
        "\n",
        "## Merging the final df and merged df\n",
        "temp = merged.merge(final, on='VisitNumber')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "ghk3CZ3-7ml6",
        "outputId": "6037801f-2bbb-4b6a-ea6b-cd095c32f673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-69bae1b37de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## Create a new df to append a groupby series of the sum of items bought in each trip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mitems_bought_each_trip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvuoU8fEDAZG"
      },
      "source": [
        "## Final Test Set \n",
        "We will reserve 10% of our data for final validation. This will serve as our overall task performance benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aprl-kZQC-v5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_set, test_set = train_test_split(train, test_size = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN-QjMRzC_KU"
      },
      "source": [
        "##Partitioning Train/Test Sets:\n",
        "Initially, we decided that our train/test data splitting will be implemented using Kfold CV. But since we are working very large dataset (500k+ datapoints), we found we had insufficent compute resource to run too many iterations. Now, the data splitting will be done through a simple train-test partition with 70% of the datapoints in the training set and 30% in the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3OM2cpKHrx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8578a62e-4505-48ce-89ef-595ee2d661a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38,)\n",
            "(450047, 5)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = train.dropna()\n",
        "#X = temp\n",
        "y = X['TripType']\n",
        "X = X.drop(columns=['TripType'])\n",
        "X = X.drop(columns=[\"DepartmentDescription\"])\n",
        "#y = temp['TripType']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .70, shuffle=True)\n",
        "print (y_test.unique().shape)\n",
        "print(X_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIINXk_pdYWU"
      },
      "source": [
        "# Proposed Solution\n",
        "\n",
        "Our proposed solution is to use train and fit a model to the training data, evaluate its performance on the testing data, and then use cross validation to verify the accuracy of our results.\n",
        "\n",
        "Given the nature of our problem, non-binary classification of trip type into 38 distinct categories, we have isolated some models we believe would be best suited for the task:\n",
        "\n",
        "- SVM\n",
        "- Random Forest \n",
        "- Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKNmuxrbKzkd"
      },
      "source": [
        "####Note On Model Selection:\n",
        "The nature of our dataset naturally lends itself to certain models and leave other ones unviable. Because of the number of features we have and the large dataset, K-NN Classifiers would suffer. Logistic regression is another such classifer that we can rule out due to the non binary catagorization of our dataset. \n",
        "\n",
        "we also are constrained in our compute time and resource, this means that certain classifiers that have more complex training times must be evaluated more harshly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYcrqzSBdYWW"
      },
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "Given the context of our data and intended solution, we need a multiclass evaluation metric. Thus, we've chosen to evaluate the performance of our benchmark model and solution models by calculating the F1-score. The F1-score combines two metrics, precision and recall, using their harmonic mean to give one single number. Therefore, the F1-score would allow us to take both the number of false positives and false negatives of trip type classification into account. Further, the F1-score is a useful evaluation metric to compare against the accuracy scores of our proposed models, considering that we have an uneven class distribution. For this reason, we chose to calculate the F1-score with a weighted average to take into account the proportion of each trip type label in our dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf-_hZY6JLvZ"
      },
      "source": [
        "#Model Training\n",
        "In search of the optimal model, we will instantiate each of our hypothesized algorithms and perform a hyperparameter sweep on each to select the best parameters. This allows each classifier to put it's best foot forward during their eventual model comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N0PN0AqONQJ"
      },
      "source": [
        "\n",
        "#####*Note on Constrained Model Training:\n",
        "Given the compute resource contraint, we will be training all of our models with a class balanced random sample (75k samples) of the larger training dataset. This is to allow for faster training times, allowing more granular hyperparameter tuning. We will then train/test the best model on the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWeiAuXVs2aj"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fGRh7JpssCF"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#using random forest to classify data with dimensionality reduced to first feature\n",
        "rfc = RandomForestClassifier(max_depth=20)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "pred = rfc.predict(X_test)\n",
        "#pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfEpdjCfssP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ea9337-b73e-4e0f-f5a7-5e4e89cb6287"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3425948008585738"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "rfc.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Hi "
      ],
      "metadata": {
        "id": "cq7rybNj78TJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUO1fYn6s8-q"
      },
      "outputs": [],
      "source": [
        "from sklearn. metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhQOgNxLs9Ly"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(\"Overall Accuracy:\",accuracy_score(y_test, pred))\n",
        "print(\"Overall Precision:\",precision_score(y_test, pred, average='macro'))\n",
        "print(\"Overall Recall:\",recall_score(y_test, pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpC1eDt1s9R8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_test, pred)\n",
        "\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqO4OSUHvPk"
      },
      "source": [
        "###Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm9rqUMLHvXq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "reg_params = {\n",
        "\"n_estimators\": [100,200,50]\n",
        "\n",
        "},\n",
        "\n",
        "GSCV = GridSearchCV(rfc,reg_params,scoring=\"f1\",verbose =3)\n",
        "GSCV.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--uNsN8BtGsV"
      },
      "source": [
        "## Neural Networks using SKLearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NASf1yd-Gqkd"
      },
      "source": [
        "###Base Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdtSIWn9s9WU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e15162-7efc-4c61-e934-a9abaa562171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([40, 40, 40, ..., 40, 40, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "NN = MLPClassifier(hidden_layer_sizes=(150,100), solver='adam', random_state=5671, activation =\"relu\", learning_rate=\"adaptive\")\n",
        "kf=KFold(5)\n",
        "NN.fit(X_train,y_train)\n",
        "#cross_validate(NN, X_train,y_train, scoring=(\"f1\",\"accuracy\"), cv=kf)\n",
        "pred = NN.predict(X_test)\n",
        "pred "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guh-WaHps9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9579425c-3890-4b45-bd69-c6ad24d15585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           3       0.00      0.00      0.00      2003\n",
            "           4       0.00      0.00      0.00       261\n",
            "           5       0.00      0.00      0.00      3367\n",
            "           6       0.00      0.00      0.00      1048\n",
            "           7       0.00      0.00      0.00      6956\n",
            "           8       0.00      0.00      0.00      6872\n",
            "           9       0.00      0.00      0.00      5051\n",
            "          12       0.00      0.00      0.00       660\n",
            "          14       0.00      0.00      0.00        13\n",
            "          15       0.00      0.00      0.00      2133\n",
            "          18       0.00      0.00      0.00       896\n",
            "          19       0.00      0.00      0.00       355\n",
            "          20       0.00      0.00      0.00       923\n",
            "          21       0.00      0.00      0.00      1229\n",
            "          22       0.00      0.00      0.00      1095\n",
            "          23       0.00      0.00      0.00        93\n",
            "          24       0.00      0.00      0.00      5368\n",
            "          25       0.00      0.00      0.00      8286\n",
            "          26       0.00      0.00      0.00       690\n",
            "          27       0.00      0.00      0.00      1364\n",
            "          28       0.00      0.00      0.00       828\n",
            "          29       0.00      0.00      0.00       630\n",
            "          30       0.00      0.00      0.00      1402\n",
            "          31       0.00      0.00      0.00       526\n",
            "          32       0.00      0.00      0.00      4124\n",
            "          33       0.00      0.00      0.00      2951\n",
            "          34       0.00      0.00      0.00      1472\n",
            "          35       0.00      0.00      0.00      3795\n",
            "          36       0.00      0.00      0.00      6587\n",
            "          37       0.00      0.00      0.00     11590\n",
            "          38       0.00      0.00      0.00      8944\n",
            "          39       0.00      0.00      0.00     28619\n",
            "          40       0.27      1.00      0.43     52156\n",
            "          41       0.00      0.00      0.00      1618\n",
            "          42       0.00      0.00      0.00      5814\n",
            "          43       0.00      0.00      0.00      1841\n",
            "          44       0.00      0.00      0.00      6102\n",
            "         999       0.00      0.00      0.00      5216\n",
            "\n",
            "    accuracy                           0.27    192878\n",
            "   macro avg       0.01      0.03      0.01    192878\n",
            "weighted avg       0.07      0.27      0.12    192878\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn. metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXKW7B-Ns9dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896cfa20-09b6-4ba5-f549-536655d369dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.2704092742562656\n",
            "Overall Precision: 0.007116033533059621\n",
            "Overall Recall: 0.02631578947368421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(\"Overall Accuracy:\",accuracy_score(y_test, pred))\n",
        "print(\"Overall Precision:\",precision_score(y_test, pred, average='macro'))\n",
        "print(\"Overall Recall:\",recall_score(y_test, pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zzWsPDjs9hD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y_test, pred)\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrV7_zjMGwlI"
      },
      "source": [
        "###Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GucOpzxs9lW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "NN_params = {\n",
        "    \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjJ0PxoHvlkY"
      },
      "source": [
        "#Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWLc1e8RtS3C"
      },
      "source": [
        "## Comparing models with cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnYub0M7s9oB"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for performance metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Import required libraries for machine learning classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# Define dictionary with performance metrics we want to use\n",
        "score = {'accuracy':make_scorer(accuracy_score), \n",
        "           'precision':make_scorer(precision_score),\n",
        "           'recall':make_scorer(recall_score), \n",
        "           'f1_score':make_scorer(f1_score)}\n",
        "\n",
        "# Initializing the machine learning classifiers\n",
        "log_model = LogisticRegression(C=1/0.1, solver='lbfgs', multi_class='auto', max_iter=10000)\n",
        "rfc_model = RandomForestClassifier()\n",
        "nn_model = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
        "\n",
        "def model_evaluation(X, y, kfolds):\n",
        "    \n",
        "    \n",
        "    # Performing cross-validation to each machine learning classifier\n",
        "    \n",
        "    rfc = cross_validate(rfc_model, X, y, cv=kfolds, scoring=score)\n",
        "    nn = cross_validate(nn_model, X, y, cv=kfolds, scoring=score)\n",
        "\n",
        "    # Create a data frame with the models performance metrics scores\n",
        "    models_scores_frame = pd.DataFrame({'Logistic Regression':[log['test_accuracy'].mean(),\n",
        "                                                               log['test_precision'].mean(),\n",
        "                                                               log['test_recall'].mean(),\n",
        "                                                               log['test_f1_score'].mean()],\n",
        "                                       \n",
        "                                      'Random Forests':[rfc['test_accuracy'].mean(),\n",
        "                                                       rfc['test_precision'].mean(),\n",
        "                                                       rfc['test_recall'].mean(),\n",
        "                                                       rfc['test_f1_score'].mean()],\n",
        "                                       \n",
        "                                      'Neural Networks':[nn['test_accuracy'].mean(),\n",
        "                                                              nn['test_precision'].mean(),\n",
        "                                                              nn['test_recall'].mean(),\n",
        "                                                              nn['test_f1_score'].mean()]},\n",
        "                                      \n",
        "                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "    \n",
        "      # Add 'Best Score' column\n",
        "    models_scores_frame['Highest Score'] = models_scores_frame.idxmax(axis=1)\n",
        "    \n",
        "    # Return models performance metrics scores data frame\n",
        "    return(models_scores_frame)\n",
        "  \n",
        "# Run models_evaluation function\n",
        "model_evaluation(X, y, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvbfQwYauxRz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_dim=5, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "NN= KerasClassifier(build_fn=baseline_model, epochs =200,batch_size=5,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = cross_val_score(, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "VtWkMO-Q3T1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u0dK9BUdYWY"
      },
      "source": [
        "# Ethics & Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp52ycowdYWZ"
      },
      "source": [
        "Our data does not involve anyones data or identity so it would be difficult to find a breach of ethics or privacy. One ethical dilemma that might arise is the increasing amount of data available to big corporations and how theyâ€™re using this big data to fine tune their products to keep the average person consuming even more. It would be good to question whether it is completely ethical for corporations to treat everyone as a number to maximize their profits.\n",
        "\n",
        "Another note on privacy is the trip type classification labels. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhplYiLCdYWZ"
      },
      "source": [
        "# Team Expectations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dqZncRZdYWa"
      },
      "source": [
        "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
        "\n",
        "- Arrange bi-weekly meetings that works with everyones schedule\n",
        "- Use a discord server to communicate with one another\n",
        "- Make use of project managment software to track progress\n",
        "- Be mindful of git -pull-push-overwrites such that no code is overwritten or needlessly repeated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndy_9D4UdYWa"
      },
      "source": [
        "# Project Timeline Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYsKpfridYWb"
      },
      "source": [
        "UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
        "\n",
        "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
        "|---|---|---|---|\n",
        "| 4/24  |  3:30 PM |  Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part | \n",
        "| 5/16  |  9 PM |  Delegate tasks for first checkpoint and discuss wrangling, cleaning, and EDA plan | Import and wrangle data, do some EDA | \n",
        "| 5/19  | 9 PM  | Edit and finalize data cleaning and wrangling/EDA  | Review/discuss EDA, debug, and submit checkpoint   |\n",
        "| 5/23  | 7 PM  | Finalize project/conclusion/discussion | Discuss conclusion   |\n",
        "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ0o-4_PdYWc"
      },
      "source": [
        "# Footnotes\n",
        "<a name=\"first\"></a>1.[^](#firstnote): Oliver Kramer. K Nearest Neighbors. https://link.springer.com/chapter/10.1007/978-3-642-38652-7_2<br> \n",
        "<a name=\"second\"></a>2.[^](#secondnote): Srivastava et al. (1999) Parallel Forumlations of Decision Tree Classfication Algorithms. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7475&rep=rep1&type=pdf<br>\n",
        "<a name=\"third\"></a>3.[^](#thirdnote): M. Pal (2005).Random forest classifier for remote sensing classification. https://www.tandfonline.com/doi/pdf/10.1080/01431160412331269698?casa_token=e78vG4sBDLcAAAAA:p9nt0mSjEMuazyQsDjprmwIIFt9aNRk9EtF7eKRyNozF6FsAskuvXKrMxnnftOK0xFjlUm5MX9g.<br>\n",
        "<a name=\"fourth\"></a>4.[^](#fourthnote): Stephan Dreiseitl and Lucila Ohno_Machado. (2002). Logistic regression and artificial neural network classification models: a methodology review. https://www.sciencedirect.com/science/article/pii/S1532046403000340.<br>\n",
        "<a name=\"fifth\"></a>5.[^](#fifthnote): Cui et al. (2018). Deep Embedding Logistic Regression. https://ieeexplore.ieee.org/document/8588790\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1rVn2mEdYWd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Project_group004.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}